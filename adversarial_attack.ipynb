{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30674,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!git clone https://github.com/shangtse/robust-physical-attack.git","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2025-05-09T10:54:23.283315Z","iopub.execute_input":"2025-05-09T10:54:23.283595Z","iopub.status.idle":"2025-05-09T10:54:25.681578Z","shell.execute_reply.started":"2025-05-09T10:54:23.283562Z","shell.execute_reply":"2025-05-09T10:54:25.680431Z"},"trusted":true},"outputs":[{"name":"stdout","text":"Cloning into 'robust-physical-attack'...\nremote: Enumerating objects: 101, done.\u001b[K\nremote: Total 101 (delta 0), reused 0 (delta 0), pack-reused 101 (from 1)\u001b[K\nReceiving objects: 100% (101/101), 12.15 MiB | 26.81 MiB/s, done.\nResolving deltas: 100% (31/31), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"!git clone https://github.com/tensorflow/models.git\n!cd models/research\n!protoc object_detection/protos/*.proto --python_out=.","metadata":{"execution":{"iopub.status.busy":"2024-04-01T05:36:57.769114Z","iopub.execute_input":"2024-04-01T05:36:57.769594Z","iopub.status.idle":"2024-04-01T05:37:19.078715Z","shell.execute_reply.started":"2024-04-01T05:36:57.769554Z","shell.execute_reply":"2024-04-01T05:37:19.077406Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd /kaggle/working/models/research","metadata":{"execution":{"iopub.status.busy":"2024-04-01T05:37:19.080264Z","iopub.execute_input":"2024-04-01T05:37:19.080576Z","iopub.status.idle":"2024-04-01T05:37:19.088038Z","shell.execute_reply.started":"2024-04-01T05:37:19.080549Z","shell.execute_reply":"2024-04-01T05:37:19.086996Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install tensorflow\n!pip install pillow\n!pip install lxml\n!pip install matplotlib\n!pip install Cython\n!pip install contextlib2\n!pip install jupyter\n!pip install pillow\n!pip install tf_slim","metadata":{"execution":{"iopub.status.busy":"2024-04-01T05:37:19.090530Z","iopub.execute_input":"2024-04-01T05:37:19.090858Z","iopub.status.idle":"2024-04-01T05:39:22.287504Z","shell.execute_reply.started":"2024-04-01T05:37:19.090825Z","shell.execute_reply":"2024-04-01T05:39:22.286367Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone --depth 1 https://github.com/tensorflow/models","metadata":{"execution":{"iopub.status.busy":"2024-04-01T05:39:22.289047Z","iopub.execute_input":"2024-04-01T05:39:22.289400Z","iopub.status.idle":"2024-04-01T05:39:26.709983Z","shell.execute_reply.started":"2024-04-01T05:39:22.289368Z","shell.execute_reply":"2024-04-01T05:39:26.708896Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%cd models/research\n!protoc object_detection/protos/*.proto --python_out=.\n!cp object_detection/packages/tf2/setup.py .\n!python -m pip install .","metadata":{"execution":{"iopub.status.busy":"2024-04-01T05:39:26.711656Z","iopub.execute_input":"2024-04-01T05:39:26.712003Z","iopub.status.idle":"2024-04-01T05:42:34.991781Z","shell.execute_reply.started":"2024-04-01T05:39:26.711971Z","shell.execute_reply":"2024-04-01T05:42:34.990587Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\n\nos.environ['PYTHONPATH'] += \":/kaggle/working/models\"\nsys.path.append(\"/kaggle/working/models\")","metadata":{"execution":{"iopub.status.busy":"2024-04-01T05:42:34.993279Z","iopub.execute_input":"2024-04-01T05:42:34.993624Z","iopub.status.idle":"2024-04-01T05:42:34.999083Z","shell.execute_reply.started":"2024-04-01T05:42:34.993595Z","shell.execute_reply":"2024-04-01T05:42:34.998128Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport time\nimport math\nimport tarfile\nimport zipfile\nfrom PIL import Image\nimport numpy as np\nimport tensorflow as tf\nimport urllib.request\n\nfrom io import StringIO\nfrom IPython.display import clear_output, Image, display, HTML\nfrom object_detection.utils import label_map_util\nfrom object_detection.utils.visualization_utils import visualize_boxes_and_labels_on_image_array\nfrom object_detection.core import target_assigner","metadata":{"execution":{"iopub.status.busy":"2024-04-01T05:42:35.000235Z","iopub.execute_input":"2024-04-01T05:42:35.000623Z","iopub.status.idle":"2024-04-01T05:42:36.838733Z","shell.execute_reply.started":"2024-04-01T05:42:35.000588Z","shell.execute_reply":"2024-04-01T05:42:36.837725Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Downloading the Model","metadata":{}},{"cell_type":"code","source":"OBJ_DETECT_API='~/data/'\n\nMODEL_NAME = 'faster_rcnn_inception_v2_coco_2017_11_08'\nMODEL_FILE = MODEL_NAME + '.tar.gz'\nDOWNLOAD_BASE = 'http://download.tensorflow.org/models/object_detection/'\n\n# Path to frozen detection graph. This is the actual model that is used for the object detection.\nPATH_TO_CKPT = MODEL_NAME + '/frozen_inference_graph.pb'\n\n# List of the strings that is used to add correct label for each box.\nPATH_TO_LABELS = '/kaggle/working/robust-physical-attack/data/mscoco_label_map.pbtxt'\n\nNUM_CLASSES = 90","metadata":{"execution":{"iopub.status.busy":"2024-04-01T05:42:36.839947Z","iopub.execute_input":"2024-04-01T05:42:36.840561Z","iopub.status.idle":"2024-04-01T05:42:36.845976Z","shell.execute_reply.started":"2024-04-01T05:42:36.840533Z","shell.execute_reply":"2024-04-01T05:42:36.845081Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"opener = urllib.request.URLopener()\nopener.retrieve(DOWNLOAD_BASE + MODEL_FILE, MODEL_FILE)\ntar_file = tarfile.open(MODEL_FILE)\nfor file in tar_file.getmembers():\n  file_name = os.path.basename(file.name)\n  if 'frozen_inference_graph.pb' in file_name:\n    tar_file.extract(file, os.getcwd())","metadata":{"execution":{"iopub.status.busy":"2024-04-01T05:42:36.849594Z","iopub.execute_input":"2024-04-01T05:42:36.849898Z","iopub.status.idle":"2024-04-01T05:42:39.725261Z","shell.execute_reply.started":"2024-04-01T05:42:36.849872Z","shell.execute_reply":"2024-04-01T05:42:39.724339Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Loading the Label Map","metadata":{}},{"cell_type":"code","source":"label_map = label_map_util.load_labelmap(PATH_TO_LABELS)\ncategories = label_map_util.convert_label_map_to_categories(label_map, max_num_classes=NUM_CLASSES, use_display_name=True)\ncategory_index = label_map_util.create_category_index(categories)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T05:42:39.726437Z","iopub.execute_input":"2024-04-01T05:42:39.726740Z","iopub.status.idle":"2024-04-01T05:42:39.741955Z","shell.execute_reply.started":"2024-04-01T05:42:39.726713Z","shell.execute_reply":"2024-04-01T05:42:39.740920Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Functions for Loading And Displaying the Image","metadata":{}},{"cell_type":"code","source":"from io import BytesIO\nimport numpy as np\nimport PIL.Image\nfrom IPython.display import display, Image\n\ndef read_image(path):\n    img = PIL.Image.open(path)\n    img = np.array(img, dtype=np.uint8)\n    return img\n\ndef showarray(a, fmt='png'):\n    a = np.uint8(a)\n    f = BytesIO()\n    PIL.Image.fromarray(a).save(f, fmt)\n    display(Image(data=f.getvalue()))\n\nimg = read_image('/kaggle/working/robust-physical-attack/data/img_stop_sign.png')\nshowarray(img)\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T05:42:39.743200Z","iopub.execute_input":"2024-04-01T05:42:39.744097Z","iopub.status.idle":"2024-04-01T05:42:39.965472Z","shell.execute_reply.started":"2024-04-01T05:42:39.744061Z","shell.execute_reply":"2024-04-01T05:42:39.964480Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Loading the Mask","metadata":{}},{"cell_type":"code","source":"# size of the perturbation we want to generate\npsize = 600","metadata":{"execution":{"iopub.status.busy":"2024-04-01T05:42:39.966652Z","iopub.execute_input":"2024-04-01T05:42:39.966945Z","iopub.status.idle":"2024-04-01T05:42:39.971414Z","shell.execute_reply.started":"2024-04-01T05:42:39.966919Z","shell.execute_reply":"2024-04-01T05:42:39.970350Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"RED_MASK = np.array(PIL.Image.fromarray(np.load('/kaggle/working/robust-physical-attack/data/stop_red_mask.npy')).resize((psize, psize)))\nshowarray(255*RED_MASK)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T05:42:39.972667Z","iopub.execute_input":"2024-04-01T05:42:39.972957Z","iopub.status.idle":"2024-04-01T05:42:40.107803Z","shell.execute_reply.started":"2024-04-01T05:42:39.972927Z","shell.execute_reply":"2024-04-01T05:42:40.106840Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"WHITE_MASK = np.array(PIL.Image.fromarray(np.load('/kaggle/working/robust-physical-attack/data/stop_white_mask.npy')).resize((psize, psize)))\nshowarray(255*WHITE_MASK)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T05:42:40.108954Z","iopub.execute_input":"2024-04-01T05:42:40.109259Z","iopub.status.idle":"2024-04-01T05:42:40.230337Z","shell.execute_reply.started":"2024-04-01T05:42:40.109233Z","shell.execute_reply":"2024-04-01T05:42:40.229371Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Loading the Model for Inference","metadata":{}},{"cell_type":"code","source":"import tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\ninference_graph = tf.Graph()\nwith inference_graph.as_default():\n    image_tensor = tf.placeholder(tf.float32, shape=(None, psize, psize, 3), name='image_tensor')\n    inference_graph_def = tf.GraphDef()\n    with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n        serialized_graph = fid.read()\n        inference_graph_def.ParseFromString(serialized_graph)\n        tf.import_graph_def(inference_graph_def, name='',\n                            input_map={'Preprocessor/map/TensorArrayStack/TensorArrayGatherV3:0':image_tensor})\n","metadata":{"execution":{"iopub.status.busy":"2024-04-01T05:42:40.231613Z","iopub.execute_input":"2024-04-01T05:42:40.231916Z","iopub.status.idle":"2024-04-01T05:42:41.798486Z","shell.execute_reply.started":"2024-04-01T05:42:40.231890Z","shell.execute_reply":"2024-04-01T05:42:41.797377Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Defining Utility Functions for Plotting","metadata":{}},{"cell_type":"code","source":"def plot_detections(img, scores=None, bboxes=None, min_threshold=0):\n    if scores is None or bboxes is None:\n\n        inference_sess = tf.Session(graph=inference_graph)\n\n        tensors = [ inference_graph.get_tensor_by_name('detection_boxes:0'),\n                    inference_graph.get_tensor_by_name('detection_scores:0'),\n                    inference_graph.get_tensor_by_name('detection_classes:0'),\n                    inference_graph.get_tensor_by_name('num_detections:0'),\n                    inference_graph.get_tensor_by_name('SecondStagePostprocessor/Reshape_4:0'),\n                    inference_graph.get_tensor_by_name('SecondStagePostprocessor/convert_scores:0') ]\n\n        feed_dict = { inference_graph.get_tensor_by_name('image_tensor:0'): np.expand_dims(img, axis=0) }\n\n        nms_bboxes, nms_scores, nms_classes, num_detections, bboxes, scores = inference_sess.run(tensors,\n                                                                                                 feed_dict)\n\n        bboxes = bboxes[0]\n        scores = scores[0]\n\n    sorted_classes = np.argsort(scores[:, 1:], axis=1)\n    sorted_scores = scores[:, 1:].copy()\n    sorted_bboxes = bboxes.copy()\n\n    for i, ordering in enumerate(sorted_classes):\n        sorted_scores[i, :] = scores[i, ordering+1]\n        sorted_bboxes[i, :] = bboxes[i, ordering, :]\n\n    sorted_classes += 1\n\n    img_viz = visualize_boxes_and_labels_on_image_array(img.copy(),\n                                                        sorted_bboxes[:, -1, :],\n                                                        sorted_classes[:, -1].astype(np.int32),\n                                                        sorted_scores[:, -1],\n                                                        category_index,\n                                                        use_normalized_coordinates=False,\n                                                        max_boxes_to_draw=sorted_scores.shape[1],\n                                                        min_score_thresh=min_threshold,\n                                                        line_thickness=1)\n    showarray(img_viz)\n\nplot_detections(img, min_threshold=0.5)","metadata":{"execution":{"iopub.status.busy":"2024-04-01T05:42:41.799960Z","iopub.execute_input":"2024-04-01T05:42:41.800751Z","iopub.status.idle":"2024-04-01T05:42:55.453472Z","shell.execute_reply.started":"2024-04-01T05:42:41.800711Z","shell.execute_reply":"2024-04-01T05:42:55.452616Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Defining the Optimization Model","metadata":{}},{"cell_type":"code","source":"import tensorflow.compat.v1 as tf\nimport tensorflow as tens\n\nclass ModelContainer():\n    def __init__(self):\n        self.graph = tf.Graph()\n        self.sess = tf.Session(graph=self.graph)\n        self.patch_shape = (psize, psize, 3)\n        self.batch_size_ = 10\n        self._make_model_and_ops(None)\n\n    def get_patch(self):\n        patch = np.round((self._run(self.clipped_patch_)+1)*(255/2.0)).astype(np.uint8)\n        patch *= RED_MASK\n        patch[patch == 0] = 255\n        return patch\n\n    def assign_patch(self, new_patch):\n        self._run(self.assign_patch_, {self.patch_placeholder_: new_patch})\n\n    def reset_patch(self):\n        self.assign_patch(np.zeros(self.patch_shape))\n          \n    def train_step(self, images, patch_transforms, second_stage_cls_labels, learning_rate=1.0,\n                   dropout=None, rpn_nms_bboxes=None, rpn_nms_indices=None, patch_loss_weight=None):\n        if (rpn_nms_bboxes is None) or \\\n           (rpn_nms_indices is None):\n            rpn_nms_bboxes, rpn_nms_indices = self.inference_rpn(images, patch_transforms)\n\n        feed_dict = { self.image_input_: images,\n                      self.patch_transforms_: patch_transforms,\n                      self.second_stage_cls_labels_: second_stage_cls_labels,\n                      self.rpn_nms_bboxes_placeholder_: rpn_nms_bboxes,\n                      self.rpn_nms_indices_placeholder_: rpn_nms_indices,\n                      self.learning_rate_: learning_rate }\n        \n        if patch_loss_weight is not None:\n            feed_dict[self.patch_loss_weight_] = patch_loss_weight\n        \n        tensors = [ self.train_op_,\n                    self.loss_,\n                    self.second_stage_cls_loss_, \n                    self.patch_loss_]\n\n        train_op, loss, second_stage_cls_loss, patch_loss = self._run(tensors, feed_dict, dropout=dropout)\n\n        return loss, second_stage_cls_loss, patch_loss\n    \n    def inference_rpn(self, images, patch_transforms):\n        feed_dict = { self.image_input_: images,\n                      self.patch_transforms_: patch_transforms }\n        \n        tensors = [self.rpn_nms_bboxes_,\n                   self.rpn_nms_indices_ ]\n\n        rpn_nms_bboxes, rpn_nms_indices = self._run(tensors, feed_dict)\n        \n        return rpn_nms_bboxes, rpn_nms_indices\n\n    def inference(self, images, patch_transforms, rpn_nms_bboxes=None, rpn_nms_indices=None):\n        if (rpn_nms_bboxes is None) or \\\n           (rpn_nms_indices is None):\n            rpn_nms_bboxes, rpn_nms_indices = self.inference_rpn(images, patch_transforms)\n\n        feed_dict = { self.image_input_: images,\n                      self.patch_transforms_: patch_transforms,\n                      self.rpn_nms_bboxes_placeholder_: rpn_nms_bboxes,\n                      self.rpn_nms_indices_placeholder_: rpn_nms_indices }\n\n        tensors = [ self.patched_input_,\n                    self.second_stage_cls_scores_,\n                    self.second_stage_loc_bboxes_ ]\n\n        patched_imgs, second_stage_cls_scores, second_stage_loc_bboxes = self._run(tensors, feed_dict)\n        patched_imgs = patched_imgs.astype(np.uint8)\n\n        plot_detections(patched_imgs[0], scores=second_stage_cls_scores[0], bboxes=second_stage_loc_bboxes[0], min_threshold=0.2)\n        \n        return patched_imgs, second_stage_cls_scores, second_stage_loc_bboxes\n\n    def _run(self, target, feed_dict=None, dropout=None):\n        if feed_dict is None:\n            feed_dict = {}\n         \n        if dropout is not None:\n            feed_dict[self.dropout_] = dropout\n    \n        return self.sess.run(target, feed_dict=feed_dict)\n    \n    def _make_model_and_ops(self, patch_val):\n        start = time.time()\n        with self.sess.graph.as_default():\n            tf.set_random_seed(1234)\n            \n            # Tensors are post-fixed with an underscore!\n            self.image_input_ = tf.placeholder(tf.float32, shape=(None, psize, psize, 3), name='image_input')\n            self.patch_transforms_ = tf.placeholder(tf.float32, shape=(None, 8), name='patch_transforms')\n\n            patch_ = tf.get_variable('patch', self.patch_shape, dtype=tf.float32, initializer=tf.zeros_initializer)\n            self.patch_placeholder_ = tf.placeholder(dtype=tf.float32, shape=self.patch_shape, name='patch_placeholder')\n            self.assign_patch_ = tf.assign(patch_, self.patch_placeholder_)\n            self.clipped_patch_ = tf.tanh(patch_)\n\n            self.dropout_ = tf.placeholder_with_default(1.0, [], name='dropout')\n            patch_with_dropout_ = tf.nn.dropout(self.clipped_patch_, keep_prob=self.dropout_)\n            patched_input_ = tf.clip_by_value(self._random_overlay(self.image_input_, patch_with_dropout_), clip_value_min=-1.0, clip_value_max=1.0)\n            patched_input_ = tf.clip_by_value(tf.image.random_brightness(patched_input_, 10.0/255), -1.0, 1.0)\n            self.patched_input_ = tf.fake_quant_with_min_max_vars((patched_input_ + 1)*127.5, min=0, max=255)\n\n            # Create placeholders for NMS RPN inputs\n            self.rpn_nms_bboxes_placeholder_ = tf.placeholder(tf.float32, shape=(None, 4), name='rpn_nms_bboxes')\n            self.rpn_nms_indices_placeholder_ = tf.placeholder(tf.int32, shape=(None), name='rpn_nms_indices')\n\n            detection_graph_def = tf.GraphDef()\n            with tf.gfile.GFile(PATH_TO_CKPT, 'rb') as fid:\n                serialized_graph = fid.read()\n                detection_graph_def.ParseFromString(serialized_graph)\n                tf.import_graph_def(detection_graph_def, name='detection',\n                                    input_map={\n                                               'Preprocessor/map/TensorArrayStack/TensorArrayGatherV3:0':self.patched_input_,\n                                               'Reshape_7:0':self.rpn_nms_bboxes_placeholder_,\n                                               'Reshape_8:0':self.rpn_nms_indices_placeholder_,\n                                              })\n\n            # Recreate tensors we just replaced in the input_map\n            self.rpn_nms_bboxes_ = tf.reshape(self.graph.get_tensor_by_name('detection/Reshape_6:0'), self.graph.get_tensor_by_name('detection/stack_3:0'), name='detection/Reshape_7')\n            self.rpn_nms_indices_ = tf.reshape(self.graph.get_tensor_by_name('detection/mul_1:0'), self.graph.get_tensor_by_name('detection/Reshape_8/shape:0'), name='detection/Reshape_8') \n\n            # Patch Loss\n            self.patch_loss_ = tf.nn.l2_loss(RED_MASK*(self.clipped_patch_ - np.tile(np.array([ 1.0, -0.9, -1]), (psize, psize, 1))))\n            self.patch_loss_weight_ = tf.placeholder_with_default(1.0, [], 'patch_loss_weight')\n\n            # Second-stage Class Loss\n            self.second_stage_cls_scores_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/convert_scores:0')\n            second_stage_cls_logits_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/scale_logits:0')\n            self.second_stage_cls_labels_ = tf.placeholder(tf.float32, shape=second_stage_cls_logits_.shape, name='second_stage_cls_labels')\n            second_stage_cls_losses_ = tf.nn.softmax_cross_entropy_with_logits_v2(labels=tf.reshape(self.second_stage_cls_labels_, (-1, self.second_stage_cls_labels_.shape[2])),\n                                                                                      logits=tf.reshape(second_stage_cls_logits_, (-1, second_stage_cls_logits_.shape[2]))) \n            second_stage_cls_losses_ = tf.reshape(second_stage_cls_losses_, (-1, self.second_stage_cls_labels_.shape[1]))\n            second_stage_cls_losses_ = tf.divide(second_stage_cls_losses_, tf.to_float(self.second_stage_cls_labels_.shape[1]))\n            self.second_stage_cls_loss_ = tf.reduce_sum(second_stage_cls_losses_)\n           \n            # Second-stage bounding boxes\n            self.second_stage_loc_bboxes_ = self.graph.get_tensor_by_name('detection/SecondStagePostprocessor/Reshape_4:0')\n    \n            # Sum of weighted losses\n            self.loss_ = self.patch_loss_*self.patch_loss_weight_ + (self.second_stage_cls_loss_)\n\n            # Train our attack by only training on the patch variable\n            self.learning_rate_ = tf.placeholder(tf.float32)\n            self.train_op_ = tf.train.GradientDescentOptimizer(self.learning_rate_).minimize(self.loss_, var_list=[patch_])\n            \n            if patch_val is not None:\n                self.assign_patch(patch_val)\n            else:\n                self.reset_patch()\n\n            elapsed = time.time() - start\n            print(\"Finished loading the model, took {:.0f}s\".format(elapsed))\n    \n\n    def _random_overlay(self, imgs, patch):    \n        red_mask = RED_MASK.astype(np.float32)\n        white_mask = WHITE_MASK.astype(np.float32)\n        \n        red_mask = tf.stack([red_mask] * self.batch_size_)\n        white_mask = tf.stack([white_mask] * self.batch_size_)\n        padded_patch = tf.stack([patch] * self.batch_size_)\n        \n        white = tf.ones_like(red_mask) * 0.95\n    \n\n# assuming self.patch_transforms_ is a transform matrix\n        self.patch_transforms_ = tens.reshape(self.patch_transforms_, [8])\n\n        red_mask = tf.image.transform(red_mask, self.patch_transforms_, interpolation='BILINEAR')\n#         red_mask = tf.contrib.image.transform(red_mask, self.patch_transforms_, 'NEAREST')\n        white_mask = tf.image.transform(white_mask, self.patch_transforms_, 'BILINEAR')\n        padded_patch = tf.image.transform(padded_patch, self.patch_transforms_, 'BILINEAR')\n\n        inverted_mask = (1 - red_mask - white_mask)\n\n        return white * white_mask + imgs * inverted_mask + padded_patch * red_mask\n    \n\n    def _transform_vector(self, width, x_shift, y_shift, im_scale, rot_in_degrees):\n        \"\"\"\n        If one row of transforms is [a0, a1, a2, b0, b1, b2, c0, c1], \n        then it maps the output point (x, y) to a transformed input point \n        (x', y') = ((a0 x + a1 y + a2) / k, (b0 x + b1 y + b2) / k), \n        where k = c0 x + c1 y + 1. \n        The transforms are inverted compared to the transform mapping input points to output points.\n        \"\"\"\n\n        rot = float(rot_in_degrees) / 90. * (math.pi/2)\n\n        # Standard rotation matrix\n        # (use negative rot because tf.contrib.image.transform will do the inverse)\n        rot_matrix = np.array(\n            [[math.cos(-rot), -math.sin(-rot)],\n            [math.sin(-rot), math.cos(-rot)]]\n        )\n\n        # Scale it\n        # (use inverse scale because tf.contrib.image.transform will do the inverse)\n        inv_scale = 1. / im_scale \n        xform_matrix = rot_matrix * inv_scale\n        a0, a1 = xform_matrix[0]\n        b0, b1 = xform_matrix[1]\n\n        # At this point, the image will have been rotated around the top left corner,\n        # rather than around the center of the image. \n        #\n        # To fix this, we will see where the center of the image got sent by our transform,\n        # and then undo that as part of the translation we apply.\n        x_origin = float(width) / 2\n        y_origin = float(width) / 2\n\n        x_origin_shifted, y_origin_shifted = np.matmul(\n            xform_matrix,\n            np.array([x_origin, y_origin]),\n        )\n\n        x_origin_delta = x_origin - x_origin_shifted\n        y_origin_delta = y_origin - y_origin_shifted\n\n        # Combine our desired shifts with the rotation-induced undesirable shift\n        a2 = x_origin_delta - (x_shift/(2*im_scale))\n        b2 = y_origin_delta - (y_shift/(2*im_scale))\n\n        # Return these values in the order that tf.contrib.image.transform expects\n        return np.array([a0, a1, a2, b0, b1, b2, 0, 0]).astype(np.float32)\n\n    def generate_random_transformation(self, scale_min=0.2, scale_max=0.6, width=psize, max_rotation=20):\n        im_scale = np.random.uniform(low=scale_min, high=scale_max)\n\n        padding_after_scaling = (1-im_scale) * width\n        x_delta = np.random.uniform(-padding_after_scaling, padding_after_scaling)\n        y_delta = np.random.uniform(-padding_after_scaling, padding_after_scaling)\n\n        rot = np.random.uniform(-max_rotation, max_rotation)\n\n        return self._transform_vector(width, \n                                      x_shift=x_delta,\n                                      y_shift=y_delta,\n                                      im_scale=im_scale, \n                                      rot_in_degrees=rot)    \n\nmodel = ModelContainer()","metadata":{"execution":{"iopub.status.busy":"2024-04-01T05:47:16.895654Z","iopub.execute_input":"2024-04-01T05:47:16.896051Z","iopub.status.idle":"2024-04-01T05:47:17.356434Z","shell.execute_reply.started":"2024-04-01T05:47:16.896015Z","shell.execute_reply":"2024-04-01T05:47:17.355177Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Creating the Target Labels","metadata":{}},{"cell_type":"code","source":"TARGET_CLASS = 1 # target class: person\n\ndef create_target_labels(scores, from_class, to_class):\n    target_labels = np.zeros_like(scores)\n    classes = np.argmax(scores[:, :, 1:], axis=2)+1\n\n    for i, _ in enumerate(classes):\n        for j, cls in enumerate(classes[i]):\n            cls = to_class # Just perturb all of them!\n            target_labels[i, j, cls] = 1\n\n    return target_labels\n\n# use half white images and half random noise images as the background images for the optimization\nwhite_imgs = np.ones((int(model.batch_size_ / 2), psize, psize, 3))\nnoisy_imgs = np.random.rand(int(model.batch_size_ / 2), psize, psize, 3) * 2 - 1.0\nbg_imgs = np.concatenate([ noisy_imgs, white_imgs])\n\npatch_transformations = np.zeros((model.batch_size_, 8))\nfor i in range(patch_transformations.shape[0]):\n    patch_transformations[i, :] = model.generate_random_transformation()\n\n_, scores, _ = model.inference(bg_imgs, patch_transformations)\ntarget_labels = create_target_labels(scores, 13, TARGET_CLASS)","metadata":{"execution":{"iopub.status.busy":"2024-03-28T10:06:56.989989Z","iopub.execute_input":"2024-03-28T10:06:56.990367Z","iopub.status.idle":"2024-03-28T10:07:25.552085Z","shell.execute_reply.started":"2024-03-28T10:06:56.990338Z","shell.execute_reply":"2024-03-28T10:07:25.551167Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Optimizing the Model","metadata":{}},{"cell_type":"code","source":"model.reset_patch()","metadata":{"execution":{"iopub.status.busy":"2024-03-28T10:07:25.553743Z","iopub.execute_input":"2024-03-28T10:07:25.554041Z","iopub.status.idle":"2024-03-28T10:07:25.563065Z","shell.execute_reply.started":"2024-03-28T10:07:25.554017Z","shell.execute_reply":"2024-03-28T10:07:25.561989Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i in range(251):\n    try:\n        # Generate random transformations\n        for j in range(patch_transformations.shape[0]):\n            patch_transformations[j, :] = model.generate_random_transformation(scale_min=0.05, scale_max=0.3)\n\n        # Update patch according to changed labels\n        loss, second_stage_cls_loss, patch_loss = model.train_step(bg_imgs,\n                                                                   patch_transformations,\n                                                                   target_labels,\n                                                                   learning_rate=1.0,\n                                                                   patch_loss_weight=0.005)\n\n        if (i % 50) == 0:\n            print('iter {} total loss: {} target loss: {} patch loss: {}'.format(i, loss, second_stage_cls_loss, patch_loss))\n            model.inference(bg_imgs, patch_transformations)\n\n\n    except KeyboardInterrupt:\n        print('iter {} total loss: {} target loss: {} patch loss: {}'.format(i, loss, second_stage_cls_loss, patch_loss))\n        break","metadata":{"execution":{"iopub.status.busy":"2024-03-28T10:07:25.564463Z","iopub.execute_input":"2024-03-28T10:07:25.564870Z","iopub.status.idle":"2024-03-28T10:11:49.858789Z","shell.execute_reply.started":"2024-03-28T10:07:25.564837Z","shell.execute_reply":"2024-03-28T10:11:49.857818Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"patch = model.get_patch()\nshowarray(PIL.Image.fromarray(patch))","metadata":{"execution":{"iopub.status.busy":"2024-03-28T10:11:49.861033Z","iopub.execute_input":"2024-03-28T10:11:49.861725Z","iopub.status.idle":"2024-03-28T10:11:50.164682Z","shell.execute_reply.started":"2024-03-28T10:11:49.861690Z","shell.execute_reply":"2024-03-28T10:11:50.163754Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}